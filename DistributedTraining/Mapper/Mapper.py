import numpy as np
import torch
import torch.nn as nn


class Mapper:
    """
    Enhanced Mapper class for facilitating deep learning.
    Supports distributed data preprocessing, feature extraction, and gradient computation.
    """

    def __init__(self, map_function, batch_size=None):
        """
        Initialize the Mapper with a custom map function and optional batch size.

        :param map_function: A function to process a single record or batch of records.
        :param batch_size: Optional. The size of each batch for batch processing.
        """
        self.map_function = map_function
        self.batch_size = batch_size

    def process(self, dataset):
        """
        Processes the input dataset and emits intermediate results.

        :param dataset: A list of records to process.
        :return: A list of key-value pairs generated by the map function.
        """
        if self.batch_size:
            # Split the dataset into batches
            return self._process_in_batches(dataset)
        else:
            # Process each record individually
            intermediate_results = []
            for record in dataset:
                intermediate_results.extend(self.map_function(record))
            return intermediate_results

    def _process_in_batches(self, dataset):
        """
        Processes the dataset in batches.

        :param dataset: A list of records to process.
        :return: A list of key-value pairs generated by the map function.
        """
        intermediate_results = []
        for i in range(0, len(dataset), self.batch_size):
            batch = dataset[i:i + self.batch_size]
            intermediate_results.extend(self.map_function(batch))
        return intermediate_results

    @staticmethod
    def example_preprocessing_map_function(batch):
        """
        An example map function for preprocessing data for deep learning.

        :param batch: A list of records, where each record is a numpy array.
        :return: Preprocessed features and labels.
        """
        # Assume batch contains tuples of (features, label)
        preprocessed_batch = []
        for features, label in batch:
            # Normalize features to [0, 1]
            normalized_features = features / 255.0
            preprocessed_batch.append((normalized_features, label))
        return preprocessed_batch

    @staticmethod
    def gradient_computation_map_function(batch, model, loss_fn):
        """
        A map function to compute gradients for a deep learning model.

        :param batch: A list of records, where each record is a tuple of (features, label).
        :param model: A deep learning model (e.g., PyTorch or TensorFlow model).
        :param loss_fn: Loss function for the model.
        :return: Gradients computed for the model parameters.
        """
        gradients = []
        for features, label in batch:
            # Convert numpy arrays to torch tensors
            features = torch.tensor(features, dtype=torch.float32)
            label = torch.tensor([label], dtype=torch.float32)

            # Perform a forward pass
            predictions = model(features)
            loss = loss_fn(predictions, label)

            # Compute gradients
            loss.backward()
            gradients.append({name: param.grad.clone() for name, param in model.named_parameters()})

            # Zero gradients to avoid accumulation
            model.zero_grad()
        return gradients


# Example Usage
if __name__ == "__main__":
    # Dummy dataset
    dataset = [
        (np.array([0.2, 0.4, 0.6]), 1),
        (np.array([0.8, 0.2, 0.1]), 0),
        (np.array([0.5, 0.5, 0.5]), 1),
    ]

    # Preprocessing Example
    mapper = Mapper(Mapper.example_preprocessing_map_function, batch_size=2)
    preprocessed_data = mapper.process(dataset)
    print("Preprocessed Data:")
    print(preprocessed_data)

    # Gradient Computation Example
    class SimpleModel(nn.Module):
        def __init__(self):
            super(SimpleModel, self).__init__()
            self.fc = nn.Linear(3, 1)

        def forward(self, x):
            return self.fc(x)

    # Instantiate model and loss function
    model = SimpleModel()
    loss_fn = nn.MSELoss()

    # Map function for gradient computation
    gradient_mapper = Mapper(lambda batch: Mapper.gradient_computation_map_function(batch, model, loss_fn), batch_size=2)
    gradients = gradient_mapper.process(dataset)
    print("Gradients:")
    for grad in gradients:
        print(grad)
